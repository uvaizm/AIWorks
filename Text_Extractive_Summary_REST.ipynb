{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uvaizm/AIWorks/blob/master/Text_Extractive_Summary_REST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Cx3XNBsatJyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Dec  6 16:04:48 2018\n",
        "\n",
        "@author: muvaiz\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Nov  6 12:28:47 2018\n",
        "\n",
        "@author: muvaiz\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.probability import FreqDist\n",
        "from heapq import nlargest\n",
        "from collections import defaultdict\n",
        "\n",
        "from flask import Flask, request\n",
        "from flask_restful import Resource, Api, reqparse\n",
        "from sqlalchemy import create_engine\n",
        "from json import dumps\n",
        "from flask_jsonpify import jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "api = Api(app)\n",
        "\n",
        "parser = reqparse.RequestParser()\n",
        "#parser.add_argument('task')\n",
        "parser.add_argument('chat', help='Text to summarize')\n",
        "parser.add_argument('length', type=int, help='Number of sentences to return')\n",
        "\n",
        "'''def parse_arguments():\n",
        "    \"\"\" Parse command line arguments \"\"\" \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('filepath', help='File name of text to summarize')\n",
        "    parser.add_argument('-l', '--length', default=4, help='Number of sentences to return')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "'''\n",
        "\n",
        "def read_file(text):\n",
        "    \"\"\" Read the file at designated path and throw exception if unable to do so \"\"\" \n",
        "    try:\n",
        "        formatted_text=[]\n",
        "        #with open(path, 'r') as file:\n",
        "        #    text=file.read()\n",
        "        text_lines=text.split(\"\\n\")\n",
        "        for line in text_lines:\n",
        "            if line[-1] not in list(punctuation):\n",
        "                line=line+\".\"\n",
        "                \n",
        "            formatted_text.append(line)\n",
        "        text=\"\\n\".join(formatted_text)\n",
        "        print(\"Conversation : \")\n",
        "        print(text)\n",
        "        return text\n",
        "    \n",
        "    except IOError as e:\n",
        "        print(\"Fatal Error: File ({}) could not be locaeted or is not readable.\".format(path))\n",
        "\n",
        "def sanitize_input(data):\n",
        "    \"\"\" \n",
        "    Currently just a whitespace remover. More thought will have to be given with how \n",
        "    to handle sanitzation and encoding in a way that most text files can be successfully\n",
        "    parsed\n",
        "    \"\"\"\n",
        "    replace = {\n",
        "        ord('\\f') : ' ',\n",
        "        ord('\\t') : ' ',\n",
        "        ord('\\n') : ' ',\n",
        "        ord('\\r') : None\n",
        "    }\n",
        "\n",
        "    return data.translate(replace)\n",
        "\n",
        "def tokenize_content(content):\n",
        "    stop_words=list(set(stopwords.words('english')))+list(punctuation)\n",
        "    words=word_tokenize(content.lower())\n",
        "    \n",
        "    return([sent_tokenize(content),[word for word in words if word not in stop_words] ])\n",
        "\n",
        "def score_tokens(filterd_words, sentence_tokens):\n",
        "    \"\"\"\n",
        "    Builds a frequency map based on the filtered list of words and \n",
        "    uses this to produce a map of each sentence and its total score\n",
        "    \"\"\"\n",
        "    word_freq = FreqDist(filterd_words)\n",
        "\n",
        "    ranking = defaultdict(int)\n",
        "\n",
        "    for i, sentence in enumerate(sentence_tokens):\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_freq:\n",
        "                ranking[i] += word_freq[word]\n",
        "\n",
        "    return ranking\n",
        "\n",
        "def summarize(ranks, sentences, length):\n",
        "    \"\"\"\n",
        "    Utilizes a ranking map produced by score_token to extract\n",
        "    the highest ranking sentences in order after converting from\n",
        "    array to string.  \n",
        "    \"\"\"\n",
        "    if int(length) > len(sentences): \n",
        "        print(\"Error, more sentences requested than available. Use --l (--length) flag to adjust.\")\n",
        "        exit()\n",
        "\n",
        "    indexes = nlargest(int(length), ranks, key=ranks.get)\n",
        "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
        "    return ' '.join(final_sentences)\n",
        "\n",
        "class Summarizer(Resource):\n",
        "    def post(self):\n",
        "        \"\"\" Drive the process from argument to output \"\"\" \n",
        "        args = parser.parse_args()\n",
        "        content = read_file(args['chat'])\n",
        "        content = sanitize_input(content)\n",
        "    \n",
        "        sentence_tokens, word_tokens = tokenize_content(content)  \n",
        "        sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
        "        #print(\"\\nSummary :\")\n",
        "        #return self.summarize(sentence_ranks, sentence_tokens, args.length)\n",
        "        return(jsonify(summarize(sentence_ranks, sentence_tokens, args['length'])))\n",
        "        \n",
        "api.add_resource(Summarizer, '/summarizer')\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}